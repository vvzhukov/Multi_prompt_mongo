from transformers import pipeline, AutoTokenizer
from pymongo import MongoClient
from datetime import datetime
import os
import signal
import logging
import argparse
import torch

# Configure variables
DB_URI = os.getenv("MONGO_URI", "mongodb+srv://usr:zhukovitaly:N4TnJsCJQe9HPkoy@cluster0.ycbqnn4.mongodb.net/")
DB_NAME = os.getenv("DB_NAME", "prompt_multiproc")
REQUESTS_COLLECTION = os.getenv("REQUESTS_COLLECTION", "mongorequests")
BATCH_SIZE = int(os.getenv("BATCH_SIZE", 100))
WORKER_CNT = int(os.getenv("WORKER_CNT", 8))
MIN_LEN = int(os.getenv("MIN_LEN", 5))
MAX_LEN = int(os.getenv("MAX_LEN", 200))
MODEL_NAME = str(os.getenv("MODEL_NAME", "roneneldan/TinyStories-33M"))
TEMPLATE = str(os.getenv("TEMPLATE", "phi3.template"))

# Configure logging
# TODO alter logging level by another parameter
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Initialize LLM
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
# TODO model settings (custom tokenizer, parameters, and etc.)
llm_pipeline = pipeline("text-generation",
                        model=MODEL_NAME,
                        num_workers=WORKER_CNT)

# Flag for graceful shutdown
shutdown_flag = False


def read_template(template_file):
    """Read prompt template from the file"""

    try:
        with open(template_file, 'r') as file:
            template = file.read()
        return template
    except Exception as e:  # TODO proper exception handling
        logging.error(f"Error reading template file {template_file}: {e}")
        exit(1)


def apply_template(system, question):
    """Apply template to transform the prompt"""

    template = read_template(TEMPLATE)
    # TODO deal with user and assistant
    #  system parameter provides instructions or context to the language model.
    #  user parameter represents the input or query from the user.
    #  assistant parameter represents the response generated by the language model.
    #  question parameter used to form a question to the model.

    # For now, we will only replace system & question from the DB
    return template.replace("{{ system }}", system).replace("{{ question }}", question)


def signal_handler(sig, frame):
    """Catch and process termination signals"""

    global shutdown_flag
    logging.warning("Shutdown signal received. Stopping worker...")
    shutdown_flag = True


def process_requests(requests):
    """Prepare requests: reformat inputs, process batch prompts"""

    try:
        formatted_inputs = [apply_template(req.get('system', ''), req.get('input', '')) for req in requests]
        # Batching in pipeline - not automatically a win for performance. It can be either a 10x speedup or 5x slowdown
        #      depending on hardware, data and the actual model being used.
        responses = llm_pipeline(formatted_inputs,
                                 min_length=MIN_LEN,
                                 max_length=MAX_LEN,
                                 truncation=True,
                                 pad_token_id=llm_pipeline.tokenizer.eos_token_id)
        completion_time = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3] + "Z"
        return [(req['_id'], res[0]['generated_text'], completion_time) for req, res in zip(requests, responses)]
    except Exception as e:  # TODO proper exception handling
        logging.error(f"Error processing requests:{e}")
        return [(req['_id'], None, None) for req in requests]


def worker_mgr():
    """Worker manager: connects to DB, creates workers, updates DB"""

    # Catch signals
    global shutdown_flag

    # Connect to MONGO
    try:
        client = MongoClient(DB_URI)
        db = client[DB_NAME]
        req_coll = db[REQUESTS_COLLECTION]
    except Exception as e:  # TODO proper exception handling
        logging.error(f"Error connecting to DB: {DB_URI}: {e}")
        exit(1)

    while not shutdown_flag:
        # Fetch BATCH_SIZE records from MongoDB
        requests = list(req_coll.find({"status": "new"}).limit(BATCH_SIZE))
        id_list = [rec['_id'] for rec in requests]

        if requests:
            logging.info(f"Processing records: {id_list}.")
        else:
            logging.info("No more requests to process.")
            break

        # Set batch processing start time
        start_time = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3] + "Z"
        req_coll.update_many(
            {"_id": {"$in": id_list}},
            {"$set": {"time_started": start_time}}
        )

        # TODO optimize DB update (bulk)
        for recId, response, completion_time in process_requests(requests):
            req_coll.find_one_and_update(
                {"_id": recId},
                {"$set": {"status": "processed",
                          "response": response,
                          "time_completed": completion_time}},
                upsert=True  # inserting a new document if no file is found with the mentioned criteria
            )

    # Close DB connection
    client.close()


if __name__ == '__main__':
    # arguments given by specification
    parser = argparse.ArgumentParser(description="Process LLM requests from MongoDB.")
    parser.add_argument('--parallel', type=int, default=WORKER_CNT, help='Number of workers', choices=range(1, 10))
    parser.add_argument('--model', type=str, default=MODEL_NAME, help='Model name')
    parser.add_argument('--mongo_connection_string', type=str, default=DB_URI, help='MongoDB connection string')
    parser.add_argument('--template', type=str, default=TEMPLATE, help='Template name')
    # extra arguments
    parser.add_argument('--db_name', type=str, default=DB_NAME, help='Database name')
    parser.add_argument('--requests_collection', type=str, default=REQUESTS_COLLECTION, help='Requests collection name')
    parser.add_argument('--batch_size', type=int, default=BATCH_SIZE, help='Batch size (to feed the workers)',
                        choices=range(1, 1000))
    parser.add_argument('--min_len', type=int, default=MIN_LEN, help='Prompt generation parameters',
                        choices=range(0, 2000))
    parser.add_argument('--max_len', type=int, default=MAX_LEN, help='Prompt generation parameters',
                        choices=range(1, 2000))

    # TODO Sanity check for parameters (num of workers, batch size, etc.)

    args = parser.parse_args()
    WORKER_COUNT = args.parallel
    MODEL_NAME = args.model
    DB_URI = args.mongo_connection_string
    TEMPLATE = args.template
    # extra arguments
    DB_NAME = args.db_name
    REQUESTS_COLLECTION = args.requests_collection
    BATCH_SIZE = args.batch_size
    TIMEOUT = args.timeout
    MIN_LEN = args.min_len
    MAX_LEN = args.max_len

    # TODO interrupts are not working properly with the batch pipeline
    # Register signal handlers
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    logging.info(f"Starting with {WORKER_COUNT} workers and batch size of {BATCH_SIZE}...")
    logging.info(f"CUDA available: {torch.cuda.is_available()}")
    worker_mgr()
    logging.info("Worker finished.")
